---
title: "Report: PGS Catalog Calculator Pipeline"
output: 
  html_document:
    toc: true
    toc_depth: 5
    toc_float: 
      collapsed: false
date: "`r Sys.time()`"
params:
  img: PGS_Logo.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(httr)
library(jsonlite)
library(dplyr)
library(ggplot2)

log_paths <- list.files(pattern = "*_log.csv", full.names = TRUE)

read_log <- function(path) {
  log <- read.csv(path, stringsAsFactors = FALSE)
  
  return(
    log %>%
      mutate(match_type = ifelse(
        log$match_type == "",
        "no match",
        log$match_type
      )) %>%
      mutate(
        accession = ifelse(
          stringr::str_detect(accession, "^PGS"),
          stringr::str_extract(accession, "^PGS[0-9]{6}"),
          accession
        )
      ) %>%
      mutate(
        is_multiallelic =  factor(is_multiallelic, levels = c("false", "true")),
        ambiguous = factor(ambiguous, levels = c("false", "true"))
      ) %>%
      rename(sampleset = dataset)
  )
}

log <- Reduce(dplyr::bind_rows, lapply(log_paths, read_log))
```

<style>
#TOC {
  background: url("https://github.com/PGScatalog/pgsc_calc/blob/main/assets/PGS_Logo.png?raw=true");
  background-size: 33%;
  background-position: top;
  padding-top: 100px !important;
  background-repeat: no-repeat;
}
</style>

## Pipeline command 

```{r engine='bash', comment='', echo=FALSE}
cat command.txt
```

## Scoring file metadata 

<div class="alert alert-info", role="alert">
<p>Additional <a href="https://pgsc-calc.readthedocs.io/en/dev/explanation/output.html">documentation is available</a> that explains some of the terms used this report in more detail</p>
</div>

### Scoring files

```{r, echo = FALSE}
base_table <- function(log) {
  log %>%
    count(accession, name = "n_variants")
}

call_api <- function(log) {
  accessions <- trimws(unique(log$accession))
  pgs_ids <- paste(accessions, collapse = ",")
  api_url <- paste0("https://www.pgscatalog.org/rest/score/search?pgs_ids=", pgs_ids)
  res <- GET(api_url)
  return(fromJSON(rawToChar(res$content)))
}

add_urls <- function(df) {
  df$accession <- df$pgs_id
  df$pgs_id <- paste0('<a href="https://www.pgscatalog.org/score/', df$pgs_id, '">', df$pgs_id, '</a>')
  df$pgp_id <- paste0('<a href="https://www.pgscatalog.org/publication/', df$id, '">', df$id, '</a>')
  df$mapped <- paste0('<a href="', df$url, '">', df$label, '</a>')
  df$url <- df$label <- df$id <- NULL
  return(df)
}

make_publication_table <- function(log) {
  json <- call_api(log)
  
  if (json$size == 0) {
    return(data.frame())
  }
  
  df <- data.frame(pgs_id = json$results$id) 
  pubs <- json$results$publication[, c("id", "firstauthor", "journal", "date_publication")]
  traits <- data.frame(reported = json$results$trait_reported,
                       Reduce(dplyr::bind_rows, json$results$trait_efo)[, c("label", "url")])
  
  unformatted <- dplyr::bind_cols(df, pubs, traits)
  
  unformatted$genome_build <- json$results$variants_genomebuild
  
  return(add_urls(tidyr::unite(unformatted, "Publication", firstauthor:date_publication, sep = " ")))
}

log %>%
  count(accession, name = "n_variants") %>%
  left_join(make_publication_table(log), by = "accession") %>%
  tidyr::unite("Publication", c(pgp_id, Publication), sep = " | ") %>%
  mutate(reported = paste0("<u>Reported trait</u>: ", reported),
         mapped = paste0("<u>Mapped trait</u>: ", mapped)) %>%
  tidyr::unite("Trait", c(reported, mapped), sep = "<br>") %>%
  select(accession, pgs_id, Publication, Trait, n_variants, genome_build) %>% 
  DT::datatable(
    rownames = FALSE,
    escape = FALSE,
    colnames = c(
      "Scoring file" = "accession",
      "Polygenic Score ID" = "pgs_id",
      "Number of variants" = "n_variants",
      "Genome build (reported)" = "genome_build"
    ),
    extensions = 'Buttons',
    options = list(dom = 'Bfrtip',
                   buttons = c('csv'))
  )
```

### Variant matching

<hr>

#### Parameters

```{bash match_params, echo = FALSE}
cat "params.txt"
```

```{r match_summary, echo = FALSE}
log %>%
  mutate(match_type = forcats::fct_collapse(match_type, unmatched = "no match", other_level = "matched")) %>%
  group_by(sampleset, accession, match_type, match_pass) %>%
  count() %>%
  group_by(sampleset, accession) %>%
  mutate(percent = round(n / sum(n) * 100, 1)) %>%
  arrange(accession, desc(percent)) %>%
  tidyr::pivot_wider(names_from = match_type, values_from = c(n, percent)) %>%
  replace(is.na(.), 0) -> compat
```

```{r compat_details, echo = FALSE, message = FALSE}
log %>%
  mutate(match_type = forcats::fct_collapse(match_type, unmatched = "no match", other_level = "matched")) %>%
  filter(match_pass == "true") %>%
  group_by(sampleset, accession) %>%
  count(match_type, is_multiallelic, ambiguous, .drop = FALSE) %>%
  filter(match_type == "matched") -> counts

counts %>%
  filter(is_multiallelic == "true" & match_type == "matched") %>%
  group_by(sampleset, accession) %>%
  summarise(n_matched_multiallelic = sum(n)) -> n_matched_multiallelic
  
counts %>%
  filter(ambiguous == "true" & match_type == "matched") %>%
  group_by(sampleset, accession) %>%
  summarise(n_matched_ambiguous = sum(n)) -> n_matched_ambiguous

log %>%
  group_by(sampleset) %>%
  count(accession, name = "n_variants") -> n_variants
```

<hr>

#### Summary

```{r match_table, echo = FALSE}
compat %>%
  left_join(n_matched_ambiguous, by = c("sampleset", "accession")) %>%
  left_join(n_matched_multiallelic, by = c("sampleset", "accession")) %>%
  left_join(n_variants, by = c("sampleset", "accession")) %>%
  select(sampleset,
         accession,
         n_variants,
         match_pass,
         percent_matched,
         n_matched,
         n_unmatched) %>%
  DT::datatable(rownames = FALSE,
                extensions = 'Buttons',
    options = list(dom = 'Bfrtip',
                   buttons = c('csv')),
    colnames = c(
      "Sampleset" = "sampleset",
      "Scoring file" = "accession",
      "Number of variants" = "n_variants",
      "Passed matching" = "match_pass",
      "Match %" = "percent_matched",
      "Total matched" = "n_matched",
      "Total unmatched" = "n_unmatched"
    ))
```

<hr>
#### Detailed results

```{r match_table_detailed, echo = FALSE}
log %>%
  mutate(match_type =
           suppressWarnings(
             forcats::fct_collapse(
               match_type,
               unmatched = "no match",
               match_flipped = c("refalt_flip", "altref_flip"),
               matched_no_other_allele = c("no_oa_alt", "no_oa_ref", 
                                           "no_oa_alt_flip", "no_oa_ref_flip"),
               other_level = "matched"
             )
           )) %>%
  group_by(sampleset, accession) %>%
  count(match_pass, match_type, ambiguous, is_multiallelic) %>%
  rename(is_ambiguous = ambiguous) %>%
  mutate(percent = round(n / sum(n) * 100, 2)) %>%
  arrange(accession, desc(percent)) %>%
  DT::datatable(rownames=FALSE,
                extensions = 'Buttons',
    options = list(dom = 'Bfrtip',
                   buttons = c('csv')),
    colnames = c(
      "Sampleset" = "sampleset",
      "Scoring file" = "accession",
      "Passed matching" = "match_pass",
      "Match type" = "match_type",
      "Ambiguous" = "is_ambiguous",
      "Multiallelic" = "is_multiallelic"
    ))
```

## Scores 

```{r, echo = FALSE, message = FALSE}
read_scorefiles <- function(path) {
  sampleset <- strsplit(path, split = "_")[[1]][[1]]
  df <- read.table(path, header = TRUE, comment.char = "")
  cols <- colnames(df) 
  cols[[1]] <- "IID"
  colnames(df) <- cols
  df$sampleset <- sampleset
  return(df)
}

scorefiles <- list.files(pattern = "*.sscore", all.files = TRUE)
combined_scorefile <- Reduce(dplyr::bind_rows, lapply(scorefiles, read_scorefiles))

combined_scorefile %>%
  group_by(sampleset, IID) %>%
  summarise(across(matches("SUM$"), sum, na.rm = TRUE),
            DENOM_SUM = sum(DENOM)) %>%
  ungroup() %>%
  mutate(across(matches("SUM$"), ~ .x / DENOM_SUM, .names = "{.col}_AVG")) -> scores

fn <- paste0("aggregated_scores_", gsub(" |:|-", "_", Sys.time()), ".txt.gz")
write.table(scores, gzfile(fn), col.names = TRUE, quote = FALSE, row.names = FALSE)
```

```{asis, echo = nrow(scores) < 50}
<div class="alert alert-warning", role="alert">
  <strong>⚠️ Warning: small sample size (n < 50)</strong> 
  <hr>
  <ul>
  <li>plink2 uses allele frequency data to <a href="https://www.cog-genomics.org/plink/2.0/score">mean-impute the dosages of missing genotypes</a></li>
  <li>Currently the pipeline disables mean-imputation in these small sample sets to make sure that the calculated PGS is as consistent with the genotype data as possible</li>
  <li>With a small sample size, the resulting score sums may be inconsistent between samples</li>
  <li>The average ([scorename]_AVG) may be more applicable as it calculates an average weighting over all genotypes present</li>
  </ul> 
  <hr>
  <p>
  In the future mean-imputation will be supported in small samplesets using ancestry-matched reference samplesets to ensure consistent calculation of score sums (e.g. 1000G Genomes).
  </p>
</div>
```

<div class="alert alert-success", role="alert">
<p>`r sum(grepl("*_SUM$", colnames(scores))) - 2` scores for `r nrow(scores)` samples processed</p>
</div>


```{r missing_scores, echo = FALSE}
log %>% 
  filter(match_pass == "false") %>%
  group_by(sampleset, accession) %>% 
  count(match_pass) -> missing_scores
```

```{asis, echo = nrow(missing_scores) > 0}
<div class="alert alert-danger", role="alert">
<p>Some scores are missing because they failed matching, check passed matching column 
in summary table</p>
</div>
```

### Score data {.tabset}

#### Score extract

<div class="alert alert-info", role="alert">
<p>Below is a summary of the aggregated scores, which might be useful for 
debugging.</p>
</div>


```{r, echo = FALSE}
scores %>%
  select(sampleset, IID, ends_with("SUM")) %>%
  tibble::as_tibble(.)
```

<div class="alert alert-info", role="alert">
<p>See here for an explanation of <a href="https://www.cog-genomics.org/plink/2.0/formats#sscore">plink2 column names</a>
</p>
</div>

#### Density plot

<div class="alert alert-info", role="alert">
<p>The summary density plots show up to six scoring files</p>
</div>

```{r, echo = FALSE, message=FALSE, warning=FALSE}
scores %>%
  ungroup() %>%
  select(-NAMED_ALLELE_DOSAGE_SUM) %>%
  select(IID, sampleset, ends_with("SUM")) %>%
  group_by(IID, sampleset) %>%
  tidyr::pivot_longer(cols = -group_cols()) %>%
  ungroup() %>%
  filter(name != "DENOM_SUM") %>%
  mutate(name = ifelse(
    stringr::str_detect(name, "^PGS"),
    stringr::str_extract(name, "^PGS[0-9]{6}"),
    name)) %>%
  group_by(sampleset, name) -> long_scores

group_keys(long_scores) %>%
  slice(1:6) -> score_keys

long_scores %>%
  inner_join(score_keys) %>%
  ggplot(aes(x = value, fill = sampleset)) +
      geom_density(alpha = 0.3) +
      facet_wrap(~name, ncol = 2, scales = "free_x") +
      theme_bw() +
      labs(x = "Score sum", y = "Density", title = "Distribution of scores")

ggsave("density.pdf")
```

### Get scores

All scores can also be found in "aggregated_scores.txt.gz", in the results folder 
output by the pipeline.